<!DOCTYPE html> 
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Jaime A. Undurraga" />
  <meta name="dcterms.date" content="2018-07-07" />
  <title>Binaural processing in normal and hearing impaired listeners</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="presentation_files/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="presentation_files/reveal.js-3.3.0.1/css/theme/white.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="presentation.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    
    <link href="presentation_files/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Binaural processing in normal and hearing impaired listeners</h1>
    <h2 class="author">Jaime A. Undurraga</h2>
    <h3 class="date">2018-07-07</h3>
</section>

<section><section id="introduction" class="titleslide slide level1"><h1>Introduction</h1></section><section id="cocktail-party-problem-cherryexperimentsrecognitionspeech1953" class="slide level2">
<h2>Cocktail-party problem <span class="citation" data-cites="CherryExperimentsRecognitionSpeech1953">(Cherry 1953)</span></h2>
<p>How do we recognize what one person is saying when others are speaking at the same time?</p>
<p><img src="my_figures/cocktail_party_effect.png" width="50%"></p>
</section><section id="section" class="slide level2">
<h2></h2>
<ul>
<li><p>An critial role of the auditory system is to parse the inputs from the left and right ears into auditory objects - auditory scene analysis. This is an escential role for survival, recognition, and comunication.</p></li>
<li>Binaural hearing provide us cues to estimate the relative number and location of sources and objects in the environment.</li>
<li><p>These cues also help us to estimate the dimensions and characteristics of rooms as well as hear out spearkers in the precense of interfiearing noise.</p></li>
</ul>
<p><img src="my_figures/casa_Grothe_2010.png" width="50%"></p>
<p>Grothe et al. (2010)</p>
</section></section>
<section><section id="monaural-and-binaural-cues" class="titleslide slide level1"><h1>Monaural and binaural cues</h1></section><section id="monaural" class="slide level2">
<h2>Monaural</h2>
<ul>
<li>Normal hearing listeners can segregate sounds remarkably well from monaural signals.</li>
<li>The brain can use the statistical regularities of a sound to group acoustic energy that is likely originated from the same source.</li>
</ul>
<div id="left">
<br> <br> <br>
<li class="fragment" data-fragment-index="0">
common onset
</li>
<!-- Rasch, R. A. The perception of simultaneous notes such as in polyphonic -->
<!-- music. Acustica 40, 21–33 (1978). -->
<!-- Darwin, C. J. & Ciocca, V. Grouping in pitch perception: effects of onset -->
<!-- asynchrony and ear of presentation of a mistuned component. J. Acoust. Soc. -->
<!-- Am. 91, 3381–3390 (1992). -->
<li class="fragment" data-fragment-index="1">
harmonicity
</li>
<!-- Moore, B. C. J., Glasberg, B. R. & Peters, R. W. Thresholds for hearing -->
<!-- mistuned partials as separate tones in harmonic complexes. J. Acoust. Soc. Am. -->
<!-- 80, 479–483 (1986). -->
<!-- Hartmann, W. M., McAdams, S. & Smith, B. K. Hearing a mistuned harmonic -->
<!-- in an otherwise periodic complex tone. J. Acoust. Soc. Am. 88, 1712–1724 -->
<!-- (1990). -->
<!-- Brunstrom, J. M. & Roberts, B. Effects of asynchrony and ear of presentation -->
<!-- on the pitch of mistuned partials in harmonic and frequency-shifted complex -->
<!-- tones. J. Acoust. Soc. Am. 110, 391–401 (2001). -->
<!-- Bernstein, J. G. & Oxenham, A. J. Harmonic segregation through mistuning -->
<!-- can improve fundamental frequency discrimination. J. Acoust. Soc. Am. 124, -->
<!-- 1653–1667 (2008). -->
<!-- Fishman, Y. I. & Steinschneider, M. Neural correlates of auditory scene -->
<!-- analysis based on inharmonicity in monkey primary auditory cortex. J. -->
<!-- Neurosci. 30, 12480–12494 (2010). -->
<li class="fragment" data-fragment-index="2">
repetition
</li>
<!-- Kidd, G., Mason, C. R., Deliwala, P. S. & Woods, W. S. Reducing -->
<!-- informational masking by sound segregation. J. Acoust. Soc. Am. 95, -->
<!-- 3475–3480 (1994). -->
<!-- McDermott, J. H., Wrobleski, D. & Oxenham, A. J. Recovering sound sources -->
<!-- from embedded repetition. Proc. Natl. Acad. Sci. USA 108, 1188–1193 (2011). -->
</div>
<div id="rigth">
<img class="fragment" data-fragment-index="3" src="./my_figures/spectrogram_cheap.png" width="80%" data-audio-src="./audio/cheap_clean.wav" data-audio-advance=-1>
<li class="fragment" data-fragment-index="3">
Play speech
</li>
</div>
</section><section id="harmonicity" class="slide level2">
<h2>Harmonicity</h2>
<img class="fragment" data-fragment-index="4" src="./my_figures/spectrogram_arctic_a0002.png" width="80%">
<li class="fragment" data-fragment-index="4" data-audio-src="./audio/sentences/arctic_a0002.wav" data-audio-advance="-1">
Play sentence
</li>
</section><section id="what-features-conveyed-by-speech-sounds" class="slide level2">
<h2>What features conveyed by speech sounds?</h2>
<ul>
<li>Temporal fine structure (TFS)</li>
<li>Envelope information (ENV)</li>
</ul>
<li class="fragment" data-fragment-index="0" data-audio-src="./audio/speech-davidvocoder.wav" data-audio-advance="-1">
Play 6 Channel vocoder
</li>
<li class="fragment" data-fragment-index="1" data-audio-src="./audio/david.wav" data-audio-advance="-1">
Play Original
</li>
<p><img src="my_figures/temporal-example.png" width="70%"></p>
</section><section id="binaural-cues" class="slide level2">
<h2>Binaural cues</h2>
<p>We listening to sounds, we rely on three mechanims for both localize a source and to perform sound source analysis</p>
<ul>
<li><p>Interaural level differences (ILDs)</p></li>
<li><p>Interaural time differences (ITDs)</p></li>
</ul>
</section><section id="itds-and-ilds" class="slide level2">
<h2>ITDs and ILDs</h2>
<p><img src="my_figures/itd_ild_cartoon.png" width="60%"></p>
</section><section id="ilds" class="slide level2">
<h2>ILDs</h2>
<img class="fragment" data-fragment-index="0" src="./my_figures/azimuth.png" width="40%"> <img class="fragment" data-fragment-index="1" src="./my_figures/azimuth_ir.png" width="50%">
<li class="fragment" data-fragment-index="2">
ILDs only useful for frequencies higher than 1200 Hz
</li>
<li class="fragment" data-fragment-index="3">
ILDs can be as large as 20 dB (e.g. Bronkhorst &amp; Plomp, 1988)
</li>
<li class="fragment" data-fragment-index="3">
ILDs resulting from the <strong>head shadow</strong> provide an advantage by sheltering the ear turned toward the target source from noise from the other side. However, listeners with unilateral hearing loss stragle when the target sound comes from the impaired side, specially in the prensece of background noise.
</li>
</section><section id="localization-using-itds" class="slide level2">
<h2>Localization using ITDs</h2>
<p><img class="fragment" data-fragment-index="0" src="./my_figures/azimuth_itd.png" width="40%"> <img class="fragment" data-fragment-index="1" src="./my_figures/itd_ir.png" width="50%"></p>
<li class="fragment" data-fragment-index="2">
ITDs within the physiological range experienced by human listeners about ±760 μs; (e.g. Constan and Hartmann (2003) and Hartmann and Macaulay (2014))
</li>
<li class="fragment" data-fragment-index="3">
ITDs (using fine structure) are useful for frequencies below 1500 Hz
</li>
<li class="fragment" data-fragment-index="3">
ITDs in the envelope of the signal are also used determine the location of a source in both (lower and higher frequencies)
</li>
</section><section id="section-1" class="slide level2">
<h2></h2>
<div id="left50">
<img class="fragment" data-fragment-index="0" src="./my_figures/_d_type_low_500_p1_0.00_p2_-1.57_mi_0.0_mp1_0.00_mp2_0.00_cf_500.0_mf_32.0.png" width="70%" data-audio-src="./audio/_d_type_low_500_p1_0.00_p2_-1.57_mi_0.0_mp1_0.00_mp2_0.00_cf_500.0_mf_32.0.wav" data-audio-advance=-1>
<p class="fragment" data-fragment-index="0">
500 Hz tone TFS left-to-right
</p>
<img class="fragment" data-fragment-index="1" src="./my_figures/_d_type_low_500_p1_0.00_p2_-0.00_mi_1.0_mp1_0.00_mp2_-1.57_cf_500.0_mf_32.0.png" width="70%" data-audio-src="./audio/_d_type_low_500_p1_0.00_p2_-0.00_mi_1.0_mp1_0.00_mp2_-1.57_cf_500.0_mf_32.0.wav" data-audio-advance=-1>
<p class="fragment" data-fragment-index="1">
Modulated 500 Hz tone <br> with envelope itd (ipd) left-to-right
</p>
</div>
<div id="rigth50">
<img class="fragment" data-fragment-index="2" src="./my_figures/_d_type_low_500_p1_0.00_p2_-1.57_mi_0.0_mp1_0.00_mp2_0.00_cf_4000.0_mf_32.0.png" width="35%" data-audio-src="./audio/_d_type_low_500_p1_0.00_p2_-1.57_mi_0.0_mp1_0.00_mp2_0.00_cf_4000.0_mf_32.0.wav" data-audio-advance=-1>
<p class="fragment" data-fragment-index="2">
4000 Hz tone TFS itd (ipd) left-to-rigth
</p>
<img class="fragment" data-fragment-index="3" src="./my_figures/_d_type_low_500_p1_0.00_p2_0.00_mi_1.0_mp1_0.00_mp2_-1.57_cf_4000.0_mf_32.0.png" width="35%" data-audio-src="./audio/_d_type_low_500_p1_0.00_p2_-0.00_mi_1.0_mp1_0.00_mp2_-1.57_cf_4000.0_mf_32.0.wav" data-audio-advance=-1>
<p class="fragment" data-fragment-index="3">
Modulated 4000 Hz tone <br> with envelope itd left-to-rigth
</p>
</div>
</section><section id="localization-using-itds-1" class="slide level2">
<h2>Localization using ITDs</h2>
<p><img src="my_figures/wave_length.png" width="40%"></p>
<li class="fragment" data-fragment-index="1">
ITDs are ambiguous for frequencies which wavelenghts are smaller than the head ‘diameter’.
</li>
</section><section id="localization-on-the-vertical-plane" class="slide level2">
<h2>Localization on the vertical plane</h2>
<p><img class="fragment" data-fragment-index="0" src="./my_figures/zenith.png" width="40%"> <img class="fragment" data-fragment-index="1" src="./my_figures/zenith_ir.png" width="50%"></p>
<li class="fragment" data-fragment-index="2">
The main cues are provided by diffraction patterns by the head and pinna, which result in characteristic spectral dips
</li>
<li class="fragment" data-fragment-index="3">
hearing-impaired subjects have wider cochlear filters (increasing spectral smearing), preventing them to localize a source in the vertical plane.
</li>
</section><section id="the-precedence-effect" class="slide level2">
<h2>The precedence effect</h2>
<p><img src="my_figures/precedence_effect_cartoon.png" width="80%"> <br> (Dietz et al. 2013)</p>
<li class="fragment" data-fragment-index="0">
A single auditory event is perceived at the direction of the first direct wave front (2 and 50 ms later, even when reflections are 10 dB louder)
</li>
<li class="fragment" data-fragment-index="1">
Assymetric hearing loss, hearing impairment and ageing negatively affect the precedence effect (Akeroyd and Guy, 2011). This can only partially restored by hearing aids.
</li>
</section></section>
<section><section id="binaural-processing-and-speech-understanding" class="titleslide slide level1"><h1>Binaural processing and speech understanding</h1></section><section id="binaural-redundancy" class="slide level2">
<h2>Binaural Redundancy</h2>
<ul>
<li><p>Loudness doubles when the two ears are used instead of one ear for a sound coming from the front of the listener (a single ear would require an increase of about 10 dB; Fletcher and Munson, 1933)</p></li>
<li><p>Just noticeable differences in intensity and frequency improve with signal redundancy</p></li>
<li><p>Speech recognition under the presence of background noise improves</p></li>
<li><p>hearing impairment may lead to a slightly weaker binaural benefit in patients (Dillon, 2001)</p></li>
<li><p>binaural stimulation sounds can be louder than with a monaural presentation without causing discomfort (even true for CI-treated patient)</p></li>
</ul>
</section><section id="binaural-release-from-masking-or-binaural-squelch-or-hirsh-effect" class="slide level2">
<h2>Binaural Release from Masking (or Binaural Squelch; or Hirsh effect)</h2>
<p><img class="fragment" data-fragment-index="0" src="./my_figures/bmld_cartoon_n0s0.png" width="45%" data-audio-src="./audio/convolved/sinusoidal_arctic_a0002_d_type_noise_target_0_d1_0_d2_0_snr_-10.wav" data-audio-advance=-1 data-audio-advance=-1 > <img class="fragment" data-fragment-index="1" src="./my_figures/bmld_cartoon_n0spi.png" width="45%" data-audio-src="./audio/convolved/sinusoidal_arctic_a0002_d_type_noise_target_0_d1_90_d2_-90_snr_-10.wav" data-audio-advance=-1 data-audio-advance=-1></p>
<li class="fragment" data-fragment-index="2">
Binaural release from masking may improve detection thesholds up to about 16 dB for frequencies around 250 Hz and around 3 dB at 1500 Hz
</li>
</section><section id="section-2" class="slide level2">
<h2></h2>
<p><img class="fragment" data-fragment-index="0" src="./my_figures/bmld_cartoon_srm_front.png" width="45%" data-audio-src="./audio/convolved/convolved_arctic_a0002_d_type_speech_target_0_d1_0_d2_0_snr_0.wav" data-audio-advance=-1 data-audio-advance=-1 > <img class="fragment" data-fragment-index="1" src="./my_figures/bmld_cartoon_srm.png" width="45%" data-audio-src="./audio/convolved/convolved_arctic_a0002_d_type_speech_target_0_d1_-80_d2_80_snr_0.wav" data-audio-advance=-1 data-audio-advance=-1></p>
<li class="fragment" data-fragment-index="2">
Binaural release from masking may imrprobe detection thresholds up to 12 dB for multiple speech interferers(Jones and Litovsky, 2011), and facilitates source segregation provided that streaming can build up and natural onset cues are present (Drennan, Gatehouse, and Lever, 2003).
</li>
<li class="fragment" data-fragment-index="3">
Segregation is always better for the combination of both ITDs and ILDs cues (Culling, Hawley, and Litovsky 2004)
</li>
<li class="fragment" data-fragment-index="4">
A separation of only 10° bwtween two voices is already strong enough to allow segregation (Brungart and Simpson, 2007).
</li>
<li class="fragment" data-fragment-index="5">
ITD is a critical spatial cue for sound localization and speech perception in noise (Bronkhorst &amp; Plomp, 1988; Wightman &amp; Kistler, 1992).
</li>
</section><section id="section-3" class="slide level2">
<h2></h2>
<p>Hearing-impaired listeners who typically find noisy environments disproportionately difficult for understanding speech, whether they wear a hearing aid or not (Killion 1997; Moore 1998)</p>
<p>their auditoryfilter bandwidths are wider than normal as a result of outer hair cell loss (see Moore 2008). The wider bandwidths lead to both spectral smearing of auditory features and reduced signal-to-noise ratio in the presence of background noise. Similar problems are encountered by users of cochlear implants, again owing to the limited frequency resolution available with implants (Clark 2003).</p>
<p>Plomp (1977) has calculated that when everyone at a well-attended party talks at the same level, the speech of the attended talker at a distance of 0.7 m has a signal-to-noise (S/N) ratio of approximately 0 dB—the background is as intense as the target talker</p>
<p>0 dB is sufficient to give adequate intelligibility with normal pronunciation and redun- dancy for listeners with normal hearing (Miller 1947)</p>
<p>AUDITORY SCENE ANALYSIS (Bregman 1990).</p>
<p>brain using two types of information to group together sound components that have originated from a common source: heuristics based on general properties of sound sources, and schematic knowledge about specific sounds. General properties such as the common onset and (for periodic sounds) the harmonic relations between frequency components from a single source can help partition sounds from different sources that occur simul- taneously; other properties, such as continuity of pitch, timbre, overall level and spatial location can help to track a single sound source across time</p>
<p>Bregman also recognizes that, as well as general heuristics, the brain may employ schema-based grouping</p>
</section></section>
<section><section id="what-and-where" class="titleslide slide level1"><h1>What and where</h1></section><section id="section-4" class="slide level2">
<h2></h2>
<p><img class="fragment" data-fragment-index="0" src="./my_figures/Anatomy_of_the_Human_Ear.svg" width="45%"> <img class="fragment" data-fragment-index="1" src="./my_figures/Uncoiled_cochlea_with_basilar_membrane_color.png" width="45%"> <img class="fragment" data-fragment-index="2" src="./my_figures/auditory_nerve.png" width="25%"></p>
</section><section id="section-5" class="slide level2">
<h2></h2>
<p><img class="fragment" data-fragment-index="0" src="./my_figures/spectrogram_cheap.png" width="45%"></p>
<p><img class="fragment" data-fragment-index="1" src="./my_figures/rater.png" width="45%"></p>
</section><section id="auditory-pathway" class="slide level2">
<h2>Auditory pathway</h2>
<p><img src="my_figures/auitorypath.png" width="50%"></p>
</section><section id="ild-pathway" class="slide level2">
<h2>ILD pathway</h2>
<p><img src="my_figures/ild_pathway.svg" width="80%"></p>
<p>(Grothe et al. 2010)</p>
<li class="fragment" data-fragment-index="0">
The initial site of ILD processing is generally considered to be the LSO.
</li>
<li class="fragment" data-fragment-index="1">
LSO are innervated by direct excitatory ipsilateral inputs from spherical bushy cells and inhibitory inputs indirectly originating from the globular bushy cells.
</li>
</section><section id="itd-pathway" class="slide level2">
<h2>ITD pathway</h2>
<p><img src="my_figures/itd_pathway.svg" width="80%"></p>
<p>(Grothe et al. 2010)</p>
<li class="fragment" data-fragment-index="0">
The initial site of ITD processing is considered to be the MSO.
</li>
<li class="fragment" data-fragment-index="1">
MSO are innervated by direct excitatory ipsi- anc contra-lateral inputs from spherical bushy cells and inhibitory inputs indirectly originating from the globular bushy cells.
</li>
</section></section>
<section><section id="objective-detection-of-binaural-processing-in-humans" class="titleslide slide level1"><h1>Objective detection of binaural processing in humans</h1></section><section id="electroencephalogram" class="slide level2">
<h2>Electroencephalogram</h2>
<p><img src="my_figures/Luck_2005_erp.png" width="60%"></p>
<p>(From Luck 2005)</p>
</section><section id="section-6" class="slide level2">
<h2></h2>
<ul>
<li>Interaural time differences (ITDs) are the main cue used by humans and other mammals to determine the horizontal position below 1.5 kHz.</li>
<li>ITD sensitivity is achieved by coincidence-detecting neurons in the superior olive - the first structure of binaural processing in the auditory pathway.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>a</th>
<th>b</th>
<th>c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><video width="320" height="240" controls source src="./my_figures/example-90.0_90.0.mp4" type="video/mp4"></td>
<td><video width="320" height="240" controls source src="./my_figures/example-45.0_45.0.mp4" type="video/mp4"></td>
<td><video width="320" height="240" controls source src="./my_figures/example-22.5_22.5.mp4" type="video/mp4"></td>
</tr>
</tbody>
</table>
</section><section id="section-7" class="slide level2">
<h2></h2>
<p><img src="my_figures/ild_itd_path.png" /></p>
<!-- <p class ="fragment" data-fragment-index="1"  data-audio-src="audio/D-8_0e+0_CF-5_0e+0_CP-7_9e-0_CA--7_9e-_MF-4_0e+0_MI-1_0e+0_CN-1_7e-0_F-4_4e+0_N-1_0e+0_A-1_0e+0_CP-2_0e+0_MP-0_0e+0_IT-1_0e+0_PW-5_0e-0_TA-1_0e+0__CR-4_4e+0_SW-1_0e+0__RF-0_0e+0_D-_.wav" data-audio-advance=-1 <p> -ipm-fr </p>> -->
<li class="fragment" data-audio-src="audio/david.wav" data-audio-advance="-1">
audio 1
</li>
<li class="fragment" data-audio-src="audio/D-8_0e+0_CF-5_0e+0_CP-7_9e-0_CA--7_9e-_MF-4_0e+0_MI-1_0e+0_CN-1_7e-0_F-4_4e+0_N-1_0e+0_A-1_0e+0_CP-2_0e+0_MP-0_0e+0_IT-1_0e+0_PW-5_0e-0_TA-1_0e+0__CR-4_4e+0_SW-1_0e+0__RF-0_0e+0_D-_.wav" data-audio-advance="-1">
audio 2
</li>
<!-- >- <audio data-autoplay class="fragment" data-audio-src="audio/david.wav" data-audio-advance=-1></audio> -->
</section></section>
<section><section id="models-for-itd-processing" class="titleslide slide level1"><h1>Models for ITD processing</h1></section><section id="straightness-weighting" class="slide level2">
<h2>Straightness weighting</h2>
<p><img src="my_figures/thompson_jeffress_2006.png" /></p>
<ul>
<li><p>Peaks of the cross-correlograms at –1.5 ms &lt; peaks closer to 0 - central weighting.</p></li>
<li><p>Correct localization estimated from second processing level (in the inferior colliculus, gray curve on top of each panel).</p></li>
</ul>
</section><section id="the-pi-limit" class="slide level2">
<h2>The <span class="math inline">\(\pi\)</span>-limit</h2>
<!-- - Findings in small mammals suggest that this sort of model may be implausible, even with the inclusion of straightness weighting.  -->
<p><img src="my_figures/thompson_2006_pi_limit.png" /></p>
<ul>
<li>ITD detectors in the mammalian brain restricted to <span class="math inline">\(\approx\)</span> half a cycle of best frequency.</li>
<li>Frequency-dependent weighting for centrality</li>
</ul>
</section><section id="section-8" class="slide level2">
<h2></h2>
<p><span class="citation" data-cites="thompson_representation_2006">(<span class="citeproc-not-found" data-reference-id="thompson_representation_2006"><strong>???</strong></span>)</span> <img src="my_figures/thompson_2006_2.png" /></p>
</section><section id="section-9" class="slide level2">
<h2></h2>
<ul>
<li>Inferior colliculus consistent with the <span class="math inline">\(\pi\)</span>␲-limit.</li>
<li>Cortical responses to sounds with ITDs within the <span class="math inline">\(\pi\)</span>-limit are in line with the predictions of both models.</li>
<li>However, neural activation is bilateral for “long” ITDs, despite these being perceived as clearly lateralized</li>
<li>Long ITDs leads to higher activation in cortex than processing of short ITDs.</li>
</ul>
</section><section id="section-10" class="slide level2">
<h2></h2>
<p><span class="citation" data-cites="kriegstein_responses_2008">(<span class="citeproc-not-found" data-reference-id="kriegstein_responses_2008"><strong>???</strong></span>)</span></p>
<p><img src="my_figures/kriegstein_2008_2.png" /></p>
</section></section>
<section><section id="aims" class="titleslide slide level1"><h1>Aims</h1></section><section id="section-11" class="slide level2">
<h2></h2>
<ul>
<li>to evaluate how short and long ITDs are processed in the human cortex using EEG recordings</li>
</ul>
</section><section id="methods" class="slide level2">
<h2>Methods</h2>
<p><img src="my_figures/example.png" /></p>
</section><section id="stimuli" class="slide level2">
<h2>Stimuli</h2>
<ul>
<li>500 Hz bandwidth modulated noise (40.4 Hz) centered at 500 Hz</li>
<li>Periodics interaural time moulations at 6.7 Hz (ITM-FRs)</li>
</ul>
<table>
<tbody>
<tr class="odd">
<td><img src="my_figures/stimuli_time.png" /></td>
<td><img src="my_figures/stimuli_spectrum.png" /></td>
</tr>
</tbody>
</table>
</section><section id="recordings" class="slide level2">
<h2>Recordings</h2>
<ul>
<li>10 normal hearing participants</li>
<li>66 channels Biosemi active electrode system</li>
<li>5 minutes recordings</li>
</ul>
<p>Analysis:</p>
<ul>
<li>low pass filter 60Hz, high pass filter 2Hz</li>
<li>automatic removal of bad channels</li>
<li>denoising source separation</li>
<li>weighted average</li>
</ul>
</section></section>
<section><section id="experiment-1" class="titleslide slide level1"><h1>Experiment 1</h1></section><section id="straightness-weighting-1" class="slide level2">
<h2>Straightness weighting</h2>
<table>
<tbody>
<tr class="odd">
<td><img src="my_figures/s_c_500us.png" /></td>
<td><img src="my_figures/s_c_2000us.png" /></td>
</tr>
<tr class="even">
<td><img src="my_figures/s_c_3000us.png" /></td>
<td><img src="my_figures/s_c_4000us.png" /></td>
</tr>
</tbody>
</table>
</section><section id="section-12" class="slide level2">
<h2></h2>
<p>ITM-FRs obtained by switching between:</p>
<ul>
<li>0 / 0.5 ms<br />
</li>
<li>0 / 1.5 ms</li>
<li>0 / 2.0 ms</li>
<li>0 / 3.0 ms</li>
<li>0 / 4.0 ms</li>
</ul>
</section></section>
<section><section id="results" class="titleslide slide level1"><h1>Results</h1></section><section id="example-responses" class="slide level2">
<h2>Example responses</h2>
<p><img src="my_figures/average-itds-zero-ref.png" /></p>
</section><section id="section-13" class="slide level2">
<h2></h2>
<p><img src="my_figures/itm-fr-coherence-zero-ref.png" /></p>
<ul>
<li>ANOVA indicated that factor ITD condition was significant (F(4, 35.04) = 7.5, p = 0.0001)</li>
</ul>
</section></section>
<section><section id="experiment-2-adapting-binaural-detectors" class="titleslide slide level1"><h1>Experiment 2 Adapting binaural detectors</h1></section><section id="section-14" class="slide level2">
<h2></h2>
<table>
<tbody>
<tr class="odd">
<td><img src="my_figures/pi_s_c_-500us.png" /></td>
<td><img src="my_figures/pi_s_c_500us.png" /></td>
</tr>
<tr class="even">
<td><img src="my_figures/pi_s_c_-500us.png" /></td>
<td><img src="my_figures/pi_s_c_1500us.png" /></td>
</tr>
</tbody>
</table>
</section><section id="section-15" class="slide level2">
<h2></h2>
<table>
<tbody>
<tr class="odd">
<td><img src="my_figures/pi_s_c_500us.png" /></td>
<td><img src="my_figures/pi_s_c_1500us.png" /></td>
</tr>
</tbody>
</table>
</section><section id="section-16" class="slide level2">
<h2></h2>
<p>ITM-FRs obtained by switching between:</p>
<ul>
<li>-0.5 / 0.5 ms</li>
<li>-0.5 / 1.5 ms</li>
<li>0.5 / 1.5 ms</li>
</ul>
</section></section>
<section><section id="results-1" class="titleslide slide level1"><h1>Results</h1></section><section id="section-17" class="slide level2">
<h2></h2>
<table>
<tbody>
<tr class="odd">
<td><img src="my_figures/itm-fr-adaptation.png" /></td>
<td><img src="my_figures/itm-fr-masking.png" /></td>
</tr>
</tbody>
</table>
</section></section>
<section><section id="experiment-3-coherence-changes" class="titleslide slide level1"><h1>Experiment 3 Coherence changes</h1></section><section id="section-18" class="slide level2">
<h2></h2>
<p>ITM-FR obtained by switching between</p>
<ul>
<li>-0.5 / 0.5 ms</li>
<li>-1.0 / 1.0 ms</li>
<li>-1.5 / 1.5 ms</li>
<li>-2.0 / 2.0 ms</li>
<li>-2.5 / 2.5 ms</li>
<li>-3.0 / 3.0 ms</li>
<li>-3.5 / 3.5 ms</li>
<li>-4.0 / 4.0 ms</li>
</ul>
</section></section>
<section><section id="results-2" class="titleslide slide level1"><h1>Results</h1></section><section id="section-19" class="slide level2">
<h2></h2>
<table>
<tbody>
<tr class="odd">
<td><img src="my_figures/_pi_lim_prediction.png" /></td>
<td><img src="my_figures/itm-fr-coherence.png" /></td>
</tr>
</tbody>
</table>
</section><section id="conclusions" class="slide level2">
<h2>Conclusions</h2>
<ul>
<li><p>Experiment 1 suggests that ITM-FRs do not decrease with ITD size when switching from zero.</p></li>
<li><p>Experiment 2 shows that ITM-FR adaptation pattern is in line with the <span class="math inline">\(\pi\)</span>-limit model.</p></li>
<li><p>ITM-FRs obtained with symmetric ITDs show a decaying damping-like pattern peaking at multiple periods of the center frequency.</p></li>
</ul>
</section><section id="references" class="slide level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-CherryExperimentsRecognitionSpeech1953">
<p>Cherry, E. Colin. 1953. “Some Experiments on the Recognition of Speech, with One and with Two Ears.” <em>J. Acoust. Soc. Am.</em> 25 (5): 975–79. doi:<a href="https://doi.org/10.1121/1.1907229">10.1121/1.1907229</a>.</p>
</div>
</div>
</section></section>
    </div>
  </div>

  <script src="presentation_files/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="presentation_files/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: false,
        // Opens links in an iframe preview overlay
        previewLinks: true,
        // Transition style
        transition: 'fade', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        chalkboard: {
          toggleNotesButton: true,
        },

        keyboard: {
          67: function() { RevealChalkboard.toggleNotesCanvas() },    // toggle notes canvas when 'c' is pressed
          66: function() { RevealChalkboard.toggleChalkboard() }, // toggle chalkboard when 'b' is pressed
          46: function() { RevealChalkboard.clear() },    // clear chalkboard when 'DEL' is pressed
           8: function() { RevealChalkboard.reset() },    // reset chalkboard data on current slide when 'BACKSPACE' is pressed
          68: function() { RevealChalkboard.download() }, // downlad recorded chalkboard drawing when 'd' is pressed
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'presentation_files/reveal.js-3.3.0.1/plugin/notes/notes.js', async: true },
          { src: 'presentation_files/reveal.js-3.3.0.1/plugin/zoom-js/zoom.js', async: true },
          { src: 'presentation_files/reveal.js-3.3.0.1/plugin/chalkboard/chalkboard.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>

<script>
 Reveal.initialize({
    // Optional reveal.js plugins
    dependencies: [
        { src: './reveal.js-plugins-master/audio-slideshow/slideshow-recorder.js', condition: function( ) { return !!document.body.classList; } },	
		{ src: './reveal.js-plugins-master/audio-slideshow/audio-slideshow.js', condition: function( ) { return !!document.body.classList; } },
    ], 
    audio: {
		prefix: 'audio/', 	// audio files are stored in the "audio" folder
		suffix: '.wav',		// audio files have the ".ogg" ending
		textToSpeechURL: null,  // the URL to the text to speech converter
		defaultNotes: false, 	// use slide notes as default for the text to speech converter
		defaultText: false, 	// use slide text as default for the text to speech converter
		advance: 0, 		// advance to next slide after given time in milliseconds after audio has played, use negative value to not advance 
		autoplay: false,	// automatically start slideshow
		defaultDuration: 5,	// default duration in seconds if no audio is available 
		playerOpacity: 0.05,	// opacity value of audio player if unfocused
		playerStyle: 'position: fixed; bottom: 4px; left: 25%; width: 50%; height:75px; z-index: 33;', // style used for container of audio controls 
		startAtFragment: false, // when moving to a slide, start at the current fragment or at the start of the slide
	},
});
</script>

  </body>
</html>
